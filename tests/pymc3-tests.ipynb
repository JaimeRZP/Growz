{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing as mp\n",
    "# mp.set_start_method(\"spawn\")\n",
    "# CCL openmp conflicts with pymc3 parallelization\n",
    "#####\n",
    "## %env OMP_NUM_THREADS=1\n",
    "import pymc3 as pm\n",
    "import pyccl as ccl\n",
    "import theano.tensor as tt\n",
    "from classy import Class\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Example 1 from Getting Started guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimerz/.local/lib/python3.8/site-packages/pymc3/sampling.py:466: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  warnings.warn(\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sigma, beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='24000' class='' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [24000/24000 00:03<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 4 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Example 1 from https://docs.pymc.io/notebooks/getting_started.html#Sampling-methods\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(123)\n",
    "\n",
    "# True parameter values\n",
    "alpha, sigma = 1, 1\n",
    "beta = [1, 2.5]\n",
    "\n",
    "# Size of dataset\n",
    "size = 100\n",
    "\n",
    "# Predictor variable\n",
    "X1 = np.random.randn(size)\n",
    "X2 = np.random.randn(size) * 0.2\n",
    "\n",
    "# Simulate outcome variable\n",
    "Y = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma\n",
    "\n",
    "\n",
    "\n",
    "basic_model = pm.Model()\n",
    "\n",
    "with basic_model:\n",
    "\n",
    "    # Priors for unknown model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=10)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10, shape=2)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta[0]*X1 + beta[1]*X2\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=Y)\n",
    "    \n",
    "    # Sample\n",
    "    step = pm.NUTS()\n",
    "    trace = pm.sample(5000, step=step)\n",
    "    \n",
    "#pm.traceplot(trace, compact=False);\n",
    "#pm.plot_posterior(trace)\n",
    "#pm.save_trace(trace, 'pymc3-test-save_trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pymc3' has no attribute 'gelman_rubin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8dd5dc02e4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelman_rubin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pymc3' has no attribute 'gelman_rubin'"
     ]
    }
   ],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pymc3' has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d3c8aabcdc53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pymc3' has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "pm.summary(trace).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.03495844e-02 -3.27375654e-04  1.41368366e-04 -1.02831148e-04]\n",
      " [-3.27375654e-04  8.02366808e-03  1.08077790e-03  1.63941567e-04]\n",
      " [ 1.41368366e-04  1.08077790e-03  2.66411787e-01  1.35004994e-04]\n",
      " [-1.02831148e-04  1.63941567e-04  1.35004994e-04  5.32806541e-03]]\n"
     ]
    }
   ],
   "source": [
    "with basic_model:\n",
    "    cov = pm.trace_cov(trace)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Example of defining your own likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lkl(pm.Continuous):\n",
    "    def __init__(self, mu, *args, **kwargs):\n",
    "        super(Lkl, self).__init__(*args, **kwargs)\n",
    "        self.mu = mu\n",
    "        self.mode = mu\n",
    "\n",
    "    def logp(self, value):\n",
    "        mu = self.mu\n",
    "        return beta_logp(value - mu)\n",
    "\n",
    "\n",
    "def beta_logp(value):\n",
    "    return -1.5 * np.log(1 + (value)**2)\n",
    "\n",
    "\n",
    "with pm.Model() as model:\n",
    "    beta = Lkl('slope', mu=3, testval=0)\n",
    "    gaus = pm.Normal('gaus', mu=0, sigma=1)\n",
    "    step = pm.NUTS()\n",
    "    trace = pm.sample(2000, tune=1000, init=None, step=step, cores=2)\n",
    "    \n",
    "pm.traceplot(trace)\n",
    "pm.summary(trace).round(2)\n",
    "\n",
    "#################\n",
    "\n",
    "with pm.Model() as model:\n",
    "    beta = Lkl('slope', mu=3, testval=0)\n",
    "    gaus = pm.Normal('gaus', mu=0, sigma=1)\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(2000, tune=1000, init=None, step=step, cores=2)\n",
    "    \n",
    "pm.traceplot(trace)\n",
    "pm.summary(trace).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Blackbox (external distributions that cannot use theano operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define a theano Op for our likelihood function\n",
    "class bao_lkl(tt.Op):\n",
    "\n",
    "    \"\"\"\n",
    "    Specify what type of object will be passed and returned to the Op when it is\n",
    "    called. In our case we will be passing it a vector of values (the parameters\n",
    "    that define our model) and returning a single \"scalar\" value (the\n",
    "    log-likelihood)\n",
    "    \"\"\"\n",
    "    itypes = [tt.dvector] # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar] # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise the Op with various things that our log-likelihood function\n",
    "        requires. Below are the things that are needed in this particular\n",
    "        example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "            \n",
    "        # Data from montepython_public/data/COMBINEDDR12_BAO_consensus_dM_Hz\n",
    "        self.rsfid = 147.78\n",
    "        cov = np.array([\n",
    "            [624.707, 23.729, 325.332, 8.34963, 157.386, 3.57778],\n",
    "            [23.729, 5.60873, 11.6429, 2.33996, 6.39263, 0.968056],\n",
    "            [325.332, 11.6429, 905.777, 29.3392, 515.271, 14.1013],\n",
    "            [8.34963, 2.33996, 29.3392, 5.42327, 16.1422, 2.85334],\n",
    "            [157.386, 6.39263, 515.271, 16.1422, 1375.12, 40.4327],\n",
    "            [3.57778, 0.968056, 14.1013, 2.85334, 40.4327, 6.25936]\n",
    "        ])\n",
    "        self.icov = np.linalg.inv(cov)\n",
    "        # BAO-only consensus results, Alam et al. 2016\n",
    "        self.z = np.array([0.38, 0.51, 0.61])\n",
    "        self.a = 1/(1+self.z)\n",
    "        # Vectors are multiplied by (rsfid/rs) so that: dM = dM*(rsfid/rs) \n",
    "        # and Hz = Hz*(rs/rsfid)\n",
    "        dM = np.array([1512.39, 1975.22, 2306.68])\n",
    "        Hz = np.array([81.2087, 90.9029, 98.9647])\n",
    "\n",
    "        data_vector = np.empty((dM.size + Hz.size), dtype=dM.dtype)\n",
    "        data_vector[0::2] = dM\n",
    "        data_vector[1::2] = Hz\n",
    "        \n",
    "        self.data = data_vector\n",
    "        self.model = Class()\n",
    "\n",
    "    def likelihood(self, theta):\n",
    "        if np.any(theta < 0):\n",
    "            return -np.inf\n",
    "\n",
    "        Omega_c, h = theta\n",
    "#         cosmo = ccl.Cosmology(Omega_c=Omega_c, Omega_b=0.045, h=h, sigma8=0.78, n_s=0.96,\n",
    "#                                transfer_function='boltzmann_class')\n",
    "#         Hz = ccl.background.h_over_h0(cosmo, self.a) * h * 100\n",
    "#         dM = ccl.background.comoving_angular_distance(cosmo, self.a)\n",
    "        \n",
    "#         params = {\n",
    "#         \"h\": cosmo[\"h\"],\n",
    "#         \"Omega_cdm\": cosmo[\"Omega_c\"],\n",
    "#         \"Omega_b\": cosmo[\"Omega_b\"],\n",
    "#         \"Omega_k\": cosmo[\"Omega_k\"],\n",
    "#         \"n_s\": cosmo[\"n_s\"],\n",
    "#         \"T_cmb\": cosmo['T_CMB']}\n",
    "    \n",
    "        params = {'h': h,\n",
    "                  'Omega_cdm': Omega_c}\n",
    "        \n",
    "        model = self.model\n",
    "        model.set(params)\n",
    "        try:\n",
    "            model.compute()\n",
    "        except:\n",
    "            model.struct_cleanup()\n",
    "            return -np.inf\n",
    "        rs = model.rs_drag()\n",
    "        dM = np.array([model.angular_distance(zi) * (1. + zi) for zi in self.z])\n",
    "        Hz = np.array([model.Hubble(zi) for zi in self.z]) * 2.99792458e8 / 1000.0\n",
    "        model.struct_cleanup()\n",
    "        model.empty()\n",
    "                \n",
    "        # Not sure how to compute r_s in CCL. Not included\n",
    "        th_vector = np.empty((dM.size + Hz.size), dtype=dM.dtype)\n",
    "        th_vector[0::2] = dM * self.rsfid / rs\n",
    "        th_vector[1::2] = Hz * rs / self.rsfid\n",
    "        \n",
    "        logl = -0.5 * (th_vector - self.data).dot(self.icov).dot(th_vector - self.data)\n",
    "        return logl\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        theta, = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(theta)\n",
    "\n",
    "        outputs[0][0] = np.array(logl) # output the log-likelihood\n",
    "    \n",
    "\n",
    "logl = bao_lkl()\n",
    "    \n",
    "with pm.Model():\n",
    "    # uniform priors on m and c\n",
    "    Omega_c = pm.Normal('Omega_c', mu=0.26, sigma=0.05)\n",
    "    h = pm.Normal('h', mu=0.68, sigma=0.05)\n",
    "\n",
    "    # convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable([Omega_c, h])\n",
    "\n",
    "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
    "    pm.DensityDist('likelihood', lambda v: logl(v), observed={'v': theta})\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(100, step=step, cores=2)\n",
    "\n",
    "# plot the traces\n",
    "_ = pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pm.summary(trace).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Blackbox 2: power spectra likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compute the true power spectra for a fiducial cosmology\n",
    "\n",
    "h = 0.67\n",
    "Omega_c = 0.26\n",
    "\n",
    "def compute_cl(h, Omega_c, z, pz, bz, ells):\n",
    "\n",
    "    cosmo = ccl.Cosmology(Omega_c=Omega_c, Omega_b=0.045, h=h, sigma8=0.78, n_s=0.96,\n",
    "                          transfer_function='boltzmann_class')\n",
    "    # Get tracer\n",
    "    tracer1 = ccl.NumberCountsTracer(cosmo,has_rsd=False,dndz=(z,pz),bias=(z,bz))\n",
    "    cls = ccl.angular_cl(cosmo, tracer1, tracer1, ells)\n",
    "\n",
    "    return cls\n",
    "\n",
    "\n",
    "# We don't add noise, yet\n",
    "\n",
    "class lkl(tt.Op):\n",
    "\n",
    "    \"\"\"\n",
    "    Specify what type of object will be passed and returned to the Op when it is\n",
    "    called. In our case we will be passing it a vector of values (the parameters\n",
    "    that define our model) and returning a single \"scalar\" value (the\n",
    "    log-likelihood)\n",
    "    \"\"\"\n",
    "    itypes = [tt.dvector] # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar] # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise the Op with various things that our log-likelihood function\n",
    "        requires. Below are the things that are needed in this particular\n",
    "        example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        h = 0.67\n",
    "        Omega_c = 0.26\n",
    "        \n",
    "        \n",
    "        # Load effective ells\n",
    "        fname = 'effective_ell_namaster.txt'\n",
    "        self.ells = np.loadtxt(fname)\n",
    "        # Import z, pz\n",
    "        fname = 'des_clustering/dndz_bin0.txt'\n",
    "        self.z, self.pz = np.loadtxt(fname, usecols=(1,3), unpack=True)\n",
    "        # Calculate bias\n",
    "        bias = 1.41\n",
    "        self.bz = bias*np.ones(self.z.shape)\n",
    "        \n",
    "        self.data = compute_cl(h, Omega_c, self.z, self.pz, self.bz, self.ells)\n",
    "        cov = np.load('cov_DESgc0_DESgc0_DESgc0_DESgc0.npz')['arr_0']\n",
    "        self.icov = np.linalg.inv(cov)\n",
    "            \n",
    "        print('Likelihoood initialized')\n",
    "\n",
    "    def likelihood(self, theta):\n",
    "        if np.any(theta < 0):\n",
    "            return -np.inf\n",
    "\n",
    "        Omega_c, h = theta\n",
    "\n",
    "        if (h < 0.6) or (h > 0.8):\n",
    "            return -np.inf\n",
    "        elif (Omega_c < 0.2) or (Omega_c > 0.35):\n",
    "            return -np.inf\n",
    "        \n",
    "        try:\n",
    "            cosmo = ccl.Cosmology(Omega_c=Omega_c, Omega_b=0.045, h=h, sigma8=0.78, n_s=0.96,\n",
    "                                  transfer_function='boltzmann_class')\n",
    "#             cosmo.compute_nonlin_power()\n",
    "#             sigma = ccl.background.h_over_h0(cosmo, 1) / 0.67 - 1\n",
    "#             th_vector = self.data + np.random.normal(scale=np.abs(sigma), size=self.data.size)\n",
    "            \n",
    "            tracer1 = ccl.NumberCountsTracer(cosmo,has_rsd=False,dndz=(self.z,self.pz),bias=(self.z,self.bz))\n",
    "            th_vector = ccl.angular_cl(cosmo, tracer1, tracer1, self.ells)\n",
    "        except Exception as e:\n",
    "            return -np.inf\n",
    "        \n",
    "        \n",
    "        logl = -0.5 * (th_vector - self.data).dot(self.icov).dot(th_vector - self.data)\n",
    "        return logl\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        theta, = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(theta)\n",
    "\n",
    "        outputs[0][0] = np.array(logl) # output the log-likelihood\n",
    "\n",
    "\n",
    "logl = lkl()\n",
    "\n",
    "with pm.Model():\n",
    "    # uniform priors on m and c\n",
    "    Omega_c = pm.Normal('Omega_c', mu=0.26, sigma=0.005)\n",
    "    h = pm.Normal('h', mu=0.68, sigma=0.005)\n",
    "\n",
    "    # convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable([Omega_c, h])\n",
    "\n",
    "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
    "    pm.DensityDist('likelihood', lambda v: logl(v), observed={'v': theta})\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(100, step=step, cores=2)\n",
    "\n",
    "# plot the traces\n",
    "_ = pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Blackbox 3: comparing NUTS vs MH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Following https://docs.pymc.io/notebooks/blackbox_external_likelihood.html we want to build a likelihood for a gc-gc power spectrum, using DES covmat, redshift bins, etc. as in the MP likelihood.\n",
    "\n",
    "We want to check if NUTS with the gradient function is faster or not than MH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "effective_ell_namaster.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a9b6e53ac34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtheta_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.67\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.26\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [h, Omega_c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mchi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcl_lkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.67\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.26\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0a9b6e53ac34>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Load effective ells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'effective_ell_namaster.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Import z, pz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'des_clustering/dndz_bin0.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: effective_ell_namaster.txt not found."
     ]
    }
   ],
   "source": [
    "# Define our likelihood and functions to load observational data and compute the Cl.\n",
    "import numpy as np\n",
    "import time\n",
    "from classy import CosmoComputationError\n",
    "\n",
    "def compute_cl(theta, data):\n",
    "    h = theta[0]\n",
    "    Omega_c = theta[1]\n",
    "    z = data['z']\n",
    "    bz = data['bz']\n",
    "    pz = data['pz']\n",
    "    ells = data['ells']\n",
    "    \n",
    "    cosmo = ccl.Cosmology(Omega_c=Omega_c, Omega_b=0.045, h=h, sigma8=0.78, n_s=0.96,\n",
    "                          transfer_function='boltzmann_class')\n",
    "    # Get tracer\n",
    "    tracer1 = ccl.NumberCountsTracer(cosmo,has_rsd=False,dndz=(z,pz),bias=(z,bz))\n",
    "    cls = ccl.angular_cl(cosmo, tracer1, tracer1, ells)\n",
    "\n",
    "    return cls\n",
    "\n",
    "def cl_lkl(theta, data):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        model = compute_cl(theta, data)\n",
    "    except CosmoComputationError:\n",
    "        print('Failed likelihood')\n",
    "        return -np.inf\n",
    "    icov = data['icov']\n",
    "    loglkl = -0.5*(model - data['cls']).dot(icov).dot(model - data['cls'])\n",
    "    end = time.time()\n",
    "    print('Computed likelihood in:', end - start, 's')\n",
    "    return loglkl\n",
    "\n",
    "def load_data():\n",
    "        # Load effective ells\n",
    "        fname = 'effective_ell_namaster.txt'\n",
    "        ells = np.loadtxt(fname)\n",
    "        # Import z, pz\n",
    "        fname = 'des_clustering/dndz_bin0.txt'\n",
    "        z, pz = np.loadtxt(fname, usecols=(1,3), unpack=True)\n",
    "        # Calculate bias\n",
    "        bias = 1.41\n",
    "        bz = bias*np.ones(z.shape)\n",
    "        \n",
    "        cov = np.load('cov_DESgc0_DESgc0_DESgc0_DESgc0.npz')['arr_0']\n",
    "        icov = np.linalg.inv(cov)\n",
    "                \n",
    "        cls = np.zeros(ells.size)  # Here we would load the cls gc0-gc0 file\n",
    "        data = {'ells': ells, 'cls': cls, 'z': z, 'pz': pz,\n",
    "                'bz': bz, 'icov': icov}\n",
    "        theta_fid = [0.67, 0.26]\n",
    "        cls = compute_cl(theta_fid, data)\n",
    "        data['cls'] = cls\n",
    "\n",
    "        return data\n",
    "\n",
    "theta_fid = [0.67, 0.26]  # [h, Omega_c]\n",
    "data = load_data()\n",
    "chi2 = cl_lkl([0.67, 0.26], data)\n",
    "print(chi2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "# Define the gradient function. Copied from:\n",
    "# https://docs.pymc.io/notebooks/blackbox_external_likelihood.html\n",
    "\n",
    "\n",
    "import cython\n",
    "cimport cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "def gradients(vals, func, releps=1e-3, abseps=None, mineps=1e-9, reltol=1e-3,\n",
    "              epsscale=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the partial derivatives of a function at a set of values. The\n",
    "    derivatives are calculated using the central difference, using an iterative\n",
    "    method to check that the values converge as step size decreases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vals: array_like\n",
    "        A set of values, that are passed to a function, at which to calculate\n",
    "        the gradient of that function\n",
    "    func:\n",
    "        A function that takes in an array of values.\n",
    "    releps: float, array_like, 1e-3\n",
    "        The initial relative step size for calculating the derivative.\n",
    "    abseps: float, array_like, None\n",
    "        The initial absolute step size for calculating the derivative.\n",
    "        This overrides `releps` if set.\n",
    "        `releps` is set then that is used.\n",
    "    mineps: float, 1e-9\n",
    "        The minimum relative step size at which to stop iterations if no\n",
    "        convergence is achieved.\n",
    "    epsscale: float, 0.5\n",
    "        The factor by which releps if scaled in each iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grads: array_like\n",
    "        An array of gradients for each non-fixed value.\n",
    "    \"\"\"\n",
    "\n",
    "    grads = np.zeros(len(vals))\n",
    "\n",
    "    # maximum number of times the gradient can change sign\n",
    "    flipflopmax = 10.\n",
    "\n",
    "    # set steps\n",
    "    if abseps is None:\n",
    "        if isinstance(releps, float):\n",
    "            eps = np.abs(vals)*releps\n",
    "            eps[eps == 0.] = releps  # if any values are zero set eps to releps\n",
    "            teps = releps*np.ones(len(vals))\n",
    "        elif isinstance(releps, (list, np.ndarray)):\n",
    "            if len(releps) != len(vals):\n",
    "                raise ValueError(\"Problem with input relative step sizes\")\n",
    "            eps = np.multiply(np.abs(vals), releps)\n",
    "            eps[eps == 0.] = np.array(releps)[eps == 0.]\n",
    "            teps = releps\n",
    "        else:\n",
    "            raise RuntimeError(\"Relative step sizes are not a recognised type!\")\n",
    "    else:\n",
    "        if isinstance(abseps, float):\n",
    "            eps = abseps*np.ones(len(vals))\n",
    "        elif isinstance(abseps, (list, np.ndarray)):\n",
    "            if len(abseps) != len(vals):\n",
    "                raise ValueError(\"Problem with input absolute step sizes\")\n",
    "            eps = np.array(abseps)\n",
    "        else:\n",
    "            raise RuntimeError(\"Absolute step sizes are not a recognised type!\")\n",
    "        teps = eps\n",
    "\n",
    "    # for each value in vals calculate the gradient\n",
    "    count = 0\n",
    "    for i in range(len(vals)):\n",
    "        # initial parameter diffs\n",
    "        leps = eps[i]\n",
    "        cureps = teps[i]\n",
    "\n",
    "        flipflop = 0\n",
    "\n",
    "        # get central finite difference\n",
    "        fvals = np.copy(vals)\n",
    "        bvals = np.copy(vals)\n",
    "\n",
    "        # central difference\n",
    "        fvals[i] += 0.5*leps  # change forwards distance to half eps\n",
    "        bvals[i] -= 0.5*leps  # change backwards distance to half eps\n",
    "        cdiff = (func(fvals)-func(bvals))/leps\n",
    "\n",
    "        while 1:\n",
    "            fvals[i] -= 0.5*leps  # remove old step\n",
    "            bvals[i] += 0.5*leps\n",
    "\n",
    "            # change the difference by a factor of two\n",
    "            cureps *= epsscale\n",
    "            if cureps < mineps or flipflop > flipflopmax:\n",
    "                # if no convergence set flat derivative (TODO: check if there is a better thing to do instead)\n",
    "                warnings.warn(\"Derivative calculation did not converge: setting flat derivative.\")\n",
    "                grads[count] = 0.\n",
    "                break\n",
    "            leps *= epsscale\n",
    "\n",
    "            # central difference\n",
    "            fvals[i] += 0.5*leps  # change forwards distance to half eps\n",
    "            bvals[i] -= 0.5*leps  # change backwards distance to half eps\n",
    "            cdiffnew = (func(fvals)-func(bvals))/leps\n",
    "\n",
    "            if cdiffnew == cdiff:\n",
    "                grads[count] = cdiff\n",
    "                break\n",
    "\n",
    "            # check whether previous diff and current diff are the same within reltol\n",
    "            rat = (cdiff/cdiffnew)\n",
    "            if np.isfinite(rat) and rat > 0.:\n",
    "                # gradient has not changed sign\n",
    "                if np.abs(1.-rat) < reltol:\n",
    "                    grads[count] = cdiffnew\n",
    "                    break\n",
    "                else:\n",
    "                    cdiff = cdiffnew\n",
    "                    continue\n",
    "            else:\n",
    "                cdiff = cdiffnew\n",
    "                flipflop += 1\n",
    "                continue\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define a theano Op for our likelihood function\n",
    "class LogLikeWithGrad(tt.Op):\n",
    "\n",
    "    itypes = [tt.dvector] # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar] # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, loglike, data):\n",
    "        \"\"\"\n",
    "        Initialise with various things that the function requires. Below\n",
    "        are the things that are needed in this particular example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loglike:\n",
    "            The log-likelihood (or whatever) function we've defined\n",
    "        data:\n",
    "            The \"observed\" data that our log-likelihood function takes in\n",
    "        \"\"\"\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "\n",
    "        # initialise the gradient Op (below)\n",
    "        self.logpgrad = LogLikeGrad(self.likelihood, self.data)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        theta, = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(theta, self.data)\n",
    "\n",
    "        outputs[0][0] = np.array(logl) # output the log-likelihood\n",
    "\n",
    "    def grad(self, inputs, g):\n",
    "        # the method that calculates the gradients - it actually returns the\n",
    "        # vector-Jacobian product - g[0] is a vector of parameter values\n",
    "        theta, = inputs  # our parameters\n",
    "        return [g[0]*self.logpgrad(theta)]\n",
    "\n",
    "\n",
    "class LogLikeGrad(tt.Op):\n",
    "\n",
    "    \"\"\"\n",
    "    This Op will be called with a vector of values and also return a vector of\n",
    "    values - the gradients in each dimension.\n",
    "    \"\"\"\n",
    "    itypes = [tt.dvector]\n",
    "    otypes = [tt.dvector]\n",
    "\n",
    "    def __init__(self, loglike, data):\n",
    "        \"\"\"\n",
    "        Initialise with various things that the function requires. Below\n",
    "        are the things that are needed in this particular example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loglike:\n",
    "            The log-likelihood (or whatever) function we've defined\n",
    "        data:\n",
    "            The \"observed\" data that our log-likelihood function takes in\n",
    "        \"\"\"\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        theta, = inputs\n",
    "\n",
    "        # define version of likelihood function to pass to derivative function\n",
    "        def lnlike(values):\n",
    "            return self.likelihood(values, self.data)\n",
    "\n",
    "        # calculate gradients\n",
    "        grads = gradients(theta, lnlike)\n",
    "\n",
    "        outputs[0][0] = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run the MCMC using NUTS\n",
    "# create our Op\n",
    "my_loglike = cl_lkl\n",
    "logl = LogLikeWithGrad(my_loglike, data)\n",
    "\n",
    "ndraws = 100\n",
    "nburn = 10\n",
    "# use PyMC3 to sampler from log-likelihood\n",
    "with pm.Model() as opmodel:\n",
    "    # uniform priors on m and c\n",
    "    h = pm.Uniform('h', lower=0.6, upper=0.7)\n",
    "    Omega_c = pm.Uniform('Omega_c', lower=0.22, upper=0.30)\n",
    "\n",
    "    # convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable([h, Omega_c])\n",
    "\n",
    "    # use a DensityDist\n",
    "    pm.DensityDist('likelihood', lambda v: logl(v), observed={'v': theta})\n",
    "\n",
    "    trace = pm.sample(ndraws, tune=nburn, discard_tuned_samples=True)\n",
    "\n",
    "# plot the traces\n",
    "_ = pm.traceplot(trace, lines={'h': theta_fid[0], 'Omega_c': theta_fid[1]})\n",
    "\n",
    "# put the chains in an array (for later!)\n",
    "# samples_pymc3_2 = np.vstack((trace['m'], trace['c'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run the MCMC using Metropolis-Hasting\n",
    "# create our Op\n",
    "my_loglike = cl_lkl\n",
    "logl = LogLikeWithGrad(my_loglike, data)\n",
    "\n",
    "ndraws = 100\n",
    "nburn = 10\n",
    "# use PyMC3 to sampler from log-likelihood\n",
    "with pm.Model() as opmodel:\n",
    "    # uniform priors on m and c\n",
    "    h = pm.Uniform('h', lower=0.6, upper=0.7)\n",
    "    Omega_c = pm.Uniform('Omega_c', lower=0.22, upper=0.30)\n",
    "\n",
    "    # convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable([h, Omega_c])\n",
    "\n",
    "    # use a DensityDist\n",
    "    pm.DensityDist('likelihood', lambda v: logl(v), observed={'v': theta})\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(ndraws, step=step, tune=nburn, discard_tuned_samples=True)\n",
    "\n",
    "# plot the traces\n",
    "_ = pm.traceplot(trace, lines={'h': theta_fid[0], 'Omega_c': theta_fid[1]})\n",
    "\n",
    "# put the chains in an array (for later!)\n",
    "# samples_pymc3_2 = np.vstack((trace['m'], trace['c'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ℓ_true = 1.0\n",
    "η_true = 3.0\n",
    "cov_func = η_true**2 * pm.gp.cov.Matern52(1, ℓ_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC3 built on top of Theano\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(),\n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of T distributed noise\n",
    "# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`\n",
    "σ_true = 2.0\n",
    "ν_true = 3.0\n",
    "y = f_true + σ_true * np.random.standard_t(ν_true, size=n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape is:', X.shape)\n",
    "print('f shape is:', f_true.shape)\n",
    "print('y shape is:', y.shape)\n",
    "print('cov shape is:', cov_func(X).eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "\n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "\n",
    "    f = gp.prior(\"f\", X=X)\n",
    "\n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    ν = pm.Gamma(\"ν\", alpha=2, beta=0.1)\n",
    "    j = pm.StudentT(\"y\", mu=f, lam=1.0/σ, nu=ν, observed=y)\n",
    "\n",
    "    trace = pm.sample(100, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, trace[\"f\"], X);\n",
    "\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylabel(\"True f(x)\");\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();\n",
    "\n",
    "plt.ylim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "92px",
    "width": "412px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
